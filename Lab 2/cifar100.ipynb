{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Классификация изображений с помощью сверточных нейронных сетей**\n",
    "\n",
    "В данном задании Вам необходимо разработать архитектуру сверточной ИНС, обеспечивающую наибольшую точность при ограничении на количество операций (FLOPs <= 0.707e6).\n",
    "Заготовка кода для выполнения задания приведена выше. Вашей задачей будет заполнить пропущеные места, которые отмечены ключевым словом *None*.\n",
    "Необходимая точность (accuracy) сети на датасете CIFAR100 - 30%\n",
    "Желаемая точность (accuracy) сети на датасете CIFAR100 - 45%"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install keras-flops"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:32.543598Z",
     "iopub.execute_input": "2023-01-25T12:12:32.544113Z",
     "iopub.status.idle": "2023-01-25T12:12:42.220251Z",
     "shell.execute_reply.started": "2023-01-25T12:12:32.544067Z",
     "shell.execute_reply": "2023-01-25T12:12:42.219064Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: keras-flops in /opt/conda/lib/python3.7/site-packages (0.1.2)\nRequirement already satisfied: tensorflow<3.0,>=2.2 in /opt/conda/lib/python3.7/site-packages (from keras-flops) (2.6.4)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12.1)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.37.1)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.15.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.6.3)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.20.3)\nRequirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.19.5)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.0)\nRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.1.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.3.0)\nRequirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.10.0.2)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.51.1)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.2.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.15.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (5.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.2)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow<3.0,>=2.2->keras-flops) (1.5.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (59.8.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3.7)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.35.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.8.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.6.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.28.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.13.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.26.13)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.2.0)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Импорт необходимых библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras_flops import get_flops"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:42.222770Z",
     "iopub.execute_input": "2023-01-25T12:12:42.223196Z",
     "iopub.status.idle": "2023-01-25T12:12:42.230390Z",
     "shell.execute_reply.started": "2023-01-25T12:12:42.223154Z",
     "shell.execute_reply": "2023-01-25T12:12:42.229403Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Глобальные константы\n",
    "CLASSES       = 100\n",
    "BATCH_SIZE    = 128\n",
    "LEARNING_RATE = 0.01"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:42.232239Z",
     "iopub.execute_input": "2023-01-25T12:12:42.232671Z",
     "iopub.status.idle": "2023-01-25T12:12:42.239021Z",
     "shell.execute_reply.started": "2023-01-25T12:12:42.232637Z",
     "shell.execute_reply": "2023-01-25T12:12:42.238058Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Выполните загрузку модели\n",
    "(X_train, y_train), (X_val, y_val) = tf.keras.datasets.cifar100.load_data()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:42.242197Z",
     "iopub.execute_input": "2023-01-25T12:12:42.242482Z",
     "iopub.status.idle": "2023-01-25T12:12:42.968713Z",
     "shell.execute_reply.started": "2023-01-25T12:12:42.242457Z",
     "shell.execute_reply": "2023-01-25T12:12:42.967716Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Преобразуйте метки классов в one_hot формат\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_val = tf.keras.utils.to_categorical(y_val)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:42.970347Z",
     "iopub.execute_input": "2023-01-25T12:12:42.970742Z",
     "iopub.status.idle": "2023-01-25T12:12:42.983409Z",
     "shell.execute_reply.started": "2023-01-25T12:12:42.970701Z",
     "shell.execute_reply": "2023-01-25T12:12:42.982390Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# убедитесь, что данная ячейка выполняется без ошибок\n",
    "assert X_train.shape == (50000, 32, 32, 3)\n",
    "assert X_val.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 100)\n",
    "assert y_val.shape == (10000, 100)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:42.985287Z",
     "iopub.execute_input": "2023-01-25T12:12:42.985743Z",
     "iopub.status.idle": "2023-01-25T12:12:42.990824Z",
     "shell.execute_reply.started": "2023-01-25T12:12:42.985708Z",
     "shell.execute_reply": "2023-01-25T12:12:42.989861Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Задайте архитектуру модели\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.Normalization(input_shape=(32, 32, 3)),\n",
    "\n",
    "    tf.keras.layers.SeparableConv2D(50, (3, 3), strides=2, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.SeparableConv2D(50, (3, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "    tf.keras.layers.Dense(CLASSES, activation='sigmoid')\n",
    "])"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:42.992091Z",
     "iopub.execute_input": "2023-01-25T12:12:42.993037Z",
     "iopub.status.idle": "2023-01-25T12:12:43.098894Z",
     "shell.execute_reply.started": "2023-01-25T12:12:42.992988Z",
     "shell.execute_reply": "2023-01-25T12:12:43.097936Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# вычисление количества операций\n",
    "flops = get_flops(model, batch_size=1)\n",
    "print(f\"FLOPs: {(flops / 1e6):.4f}e6\")"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:43.100476Z",
     "iopub.execute_input": "2023-01-25T12:12:43.100835Z",
     "iopub.status.idle": "2023-01-25T12:12:43.251360Z",
     "shell.execute_reply.started": "2023-01-25T12:12:43.100799Z",
     "shell.execute_reply": "2023-01-25T12:12:43.248481Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\nFLOPs: 0.7010e6\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/700.97k flops)\n  sequential_1/dense_2/MatMul (270.00k/270.00k flops)\n  sequential_1/separable_conv2d_3/separable_conv2d (180.00k/212.40k flops)\n    sequential_1/separable_conv2d_3/separable_conv2d/depthwise (32.40k/32.40k flops)\n  sequential_1/separable_conv2d_2/separable_conv2d (76.80k/90.62k flops)\n    sequential_1/separable_conv2d_2/separable_conv2d/depthwise (13.82k/13.82k flops)\n  sequential_1/dense_3/MatMul (60.00k/60.00k flops)\n  sequential_1/batch_normalization_3/FusedBatchNormV3 (25.90k/25.90k flops)\n  sequential_1/separable_conv2d_2/BiasAdd (12.80k/12.80k flops)\n  sequential_1/max_pooling2d_2/MaxPool (12.80k/12.80k flops)\n  sequential_1/batch_normalization_4/FusedBatchNormV3 (3.90k/3.90k flops)\n  sequential_1/normalization_1/sub (3.07k/3.07k flops)\n  sequential_1/normalization_1/truediv (3.07k/3.07k flops)\n  sequential_1/max_pooling2d_3/MaxPool (1.80k/1.80k flops)\n  sequential_1/separable_conv2d_3/BiasAdd (1.80k/1.80k flops)\n  sequential_1/batch_normalization_5/batchnorm/Rsqrt (600/600 flops)\n  sequential_1/batch_normalization_5/batchnorm/sub (300/300 flops)\n  sequential_1/dense_2/BiasAdd (300/300 flops)\n  sequential_1/batch_normalization_5/batchnorm/mul_2 (300/300 flops)\n  sequential_1/batch_normalization_5/batchnorm/mul_1 (300/300 flops)\n  sequential_1/batch_normalization_5/batchnorm/mul (300/300 flops)\n  sequential_1/batch_normalization_5/batchnorm/add_1 (300/300 flops)\n  sequential_1/batch_normalization_5/batchnorm/add (300/300 flops)\n  sequential_1/dense_3/BiasAdd (100/100 flops)\n  sequential_1/normalization_1/Maximum (3/3 flops)\n\n======================End of Report==========================\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2023-01-25 12:12:43.154751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 12:12:43.155587: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n2023-01-25 12:12:43.155734: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n2023-01-25 12:12:43.156298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 12:12:43.157162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 12:12:43.157939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 12:12:43.158777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 12:12:43.159498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 12:12:43.160213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n2023-01-25 12:12:43.162657: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n  function_optimizer: function_optimizer did nothing. time = 0.007ms.\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# вывод краткой информации о модели\n",
    "model.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:43.253110Z",
     "iopub.execute_input": "2023-01-25T12:12:43.253818Z",
     "iopub.status.idle": "2023-01-25T12:12:43.262308Z",
     "shell.execute_reply.started": "2023-01-25T12:12:43.253774Z",
     "shell.execute_reply": "2023-01-25T12:12:43.261192Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nnormalization_1 (Normalizati (None, 32, 32, 3)         7         \n_________________________________________________________________\nseparable_conv2d_2 (Separabl (None, 16, 16, 50)        227       \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 16, 16, 50)        200       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 16, 16, 50)        0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 8, 8, 50)          0         \n_________________________________________________________________\nseparable_conv2d_3 (Separabl (None, 6, 6, 50)          3000      \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 6, 6, 50)          200       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 6, 6, 50)          0         \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 3, 3, 50)          0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 3, 3, 50)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 450)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 300)               135300    \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 300)               1200      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 300)               0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 300)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 100)               30100     \n=================================================================\nTotal params: 170,234\nTrainable params: 169,427\nNon-trainable params: 807\n_________________________________________________________________\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# параметры данной ячейки могут быть изменены для получения более высокой точности\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=LEARNING_RATE, decay_steps=1000, decay_rate=0.5)\n",
    "    ),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:43.265944Z",
     "iopub.execute_input": "2023-01-25T12:12:43.266313Z",
     "iopub.status.idle": "2023-01-25T12:12:43.278088Z",
     "shell.execute_reply.started": "2023-01-25T12:12:43.266285Z",
     "shell.execute_reply": "2023-01-25T12:12:43.277237Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# обучения модели\n",
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=\"{epoch:02d}-{val_accuracy:.2f}.hdf5\", save_best_only=True),\n",
    "        \n",
    "    ],\n",
    "    use_multiprocessing=True,\n",
    "    workers=8,\n",
    "    epochs=256\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:12:43.279890Z",
     "iopub.execute_input": "2023-01-25T12:12:43.280484Z",
     "iopub.status.idle": "2023-01-25T12:25:55.867978Z",
     "shell.execute_reply.started": "2023-01-25T12:12:43.280448Z",
     "shell.execute_reply": "2023-01-25T12:25:55.867018Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/256\n391/391 [==============================] - 4s 8ms/step - loss: 3.4691 - accuracy: 0.1790 - val_loss: 3.7637 - val_accuracy: 0.1584\nEpoch 2/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.9104 - accuracy: 0.2774 - val_loss: 3.0504 - val_accuracy: 0.2618\nEpoch 3/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.6632 - accuracy: 0.3286 - val_loss: 2.7076 - val_accuracy: 0.3255\nEpoch 4/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.5027 - accuracy: 0.3592 - val_loss: 2.6553 - val_accuracy: 0.3377\nEpoch 5/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.3704 - accuracy: 0.3885 - val_loss: 2.5728 - val_accuracy: 0.3517\nEpoch 6/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.2794 - accuracy: 0.4060 - val_loss: 2.4447 - val_accuracy: 0.3803\nEpoch 7/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.1937 - accuracy: 0.4235 - val_loss: 2.4367 - val_accuracy: 0.3831\nEpoch 8/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.1354 - accuracy: 0.4344 - val_loss: 2.4170 - val_accuracy: 0.3855\nEpoch 9/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.0889 - accuracy: 0.4479 - val_loss: 2.4006 - val_accuracy: 0.3914\nEpoch 10/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.0499 - accuracy: 0.4585 - val_loss: 2.3875 - val_accuracy: 0.3942\nEpoch 11/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.0200 - accuracy: 0.4625 - val_loss: 2.3771 - val_accuracy: 0.3976\nEpoch 12/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9972 - accuracy: 0.4682 - val_loss: 2.3678 - val_accuracy: 0.3992\nEpoch 13/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9812 - accuracy: 0.4718 - val_loss: 2.3609 - val_accuracy: 0.4022\nEpoch 14/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9683 - accuracy: 0.4734 - val_loss: 2.3608 - val_accuracy: 0.4021\nEpoch 15/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9573 - accuracy: 0.4763 - val_loss: 2.3614 - val_accuracy: 0.4022\nEpoch 16/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9521 - accuracy: 0.4769 - val_loss: 2.3575 - val_accuracy: 0.4026\nEpoch 17/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9486 - accuracy: 0.4796 - val_loss: 2.3567 - val_accuracy: 0.4027\nEpoch 18/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9393 - accuracy: 0.4805 - val_loss: 2.3573 - val_accuracy: 0.4034\nEpoch 19/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9348 - accuracy: 0.4812 - val_loss: 2.3572 - val_accuracy: 0.4042\nEpoch 20/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.9362 - accuracy: 0.4818 - val_loss: 2.3589 - val_accuracy: 0.4039\nEpoch 21/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9330 - accuracy: 0.4836 - val_loss: 2.3571 - val_accuracy: 0.4033\nEpoch 22/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9305 - accuracy: 0.4818 - val_loss: 2.3578 - val_accuracy: 0.4042\nEpoch 23/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9237 - accuracy: 0.4831 - val_loss: 2.3578 - val_accuracy: 0.4037\nEpoch 24/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9301 - accuracy: 0.4828 - val_loss: 2.3577 - val_accuracy: 0.4038\nEpoch 25/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9231 - accuracy: 0.4831 - val_loss: 2.3575 - val_accuracy: 0.4044\nEpoch 26/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9314 - accuracy: 0.4820 - val_loss: 2.3573 - val_accuracy: 0.4040\nEpoch 27/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9235 - accuracy: 0.4816 - val_loss: 2.3573 - val_accuracy: 0.4046\nEpoch 28/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9308 - accuracy: 0.4823 - val_loss: 2.3575 - val_accuracy: 0.4041\nEpoch 29/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9207 - accuracy: 0.4839 - val_loss: 2.3577 - val_accuracy: 0.4034\nEpoch 30/256\n391/391 [==============================] - 4s 9ms/step - loss: 1.9194 - accuracy: 0.4871 - val_loss: 2.3575 - val_accuracy: 0.4047\nEpoch 31/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9274 - accuracy: 0.4827 - val_loss: 2.3574 - val_accuracy: 0.4048\nEpoch 32/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9263 - accuracy: 0.4847 - val_loss: 2.3572 - val_accuracy: 0.4037\nEpoch 33/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9227 - accuracy: 0.4850 - val_loss: 2.3572 - val_accuracy: 0.4042\nEpoch 34/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9189 - accuracy: 0.4849 - val_loss: 2.3577 - val_accuracy: 0.4042\nEpoch 35/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9196 - accuracy: 0.4870 - val_loss: 2.3574 - val_accuracy: 0.4047\nEpoch 36/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9301 - accuracy: 0.4831 - val_loss: 2.3573 - val_accuracy: 0.4041\nEpoch 37/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9237 - accuracy: 0.4854 - val_loss: 2.3579 - val_accuracy: 0.4045\nEpoch 38/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9229 - accuracy: 0.4851 - val_loss: 2.3571 - val_accuracy: 0.4043\nEpoch 39/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9259 - accuracy: 0.4822 - val_loss: 2.3572 - val_accuracy: 0.4038\nEpoch 40/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9209 - accuracy: 0.4878 - val_loss: 2.3572 - val_accuracy: 0.4047\nEpoch 41/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9262 - accuracy: 0.4832 - val_loss: 2.3573 - val_accuracy: 0.4042\nEpoch 42/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9305 - accuracy: 0.4858 - val_loss: 2.3573 - val_accuracy: 0.4043\nEpoch 43/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9277 - accuracy: 0.4830 - val_loss: 2.3572 - val_accuracy: 0.4040\nEpoch 44/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9284 - accuracy: 0.4833 - val_loss: 2.3571 - val_accuracy: 0.4042\nEpoch 45/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9282 - accuracy: 0.4829 - val_loss: 2.3573 - val_accuracy: 0.4038\nEpoch 46/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9252 - accuracy: 0.4838 - val_loss: 2.3573 - val_accuracy: 0.4043\nEpoch 47/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9215 - accuracy: 0.4850 - val_loss: 2.3576 - val_accuracy: 0.4046\nEpoch 48/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9260 - accuracy: 0.4842 - val_loss: 2.3577 - val_accuracy: 0.4039\nEpoch 49/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9219 - accuracy: 0.4841 - val_loss: 2.3573 - val_accuracy: 0.4048\nEpoch 50/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9271 - accuracy: 0.4846 - val_loss: 2.3570 - val_accuracy: 0.4055\nEpoch 51/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9233 - accuracy: 0.4840 - val_loss: 2.3573 - val_accuracy: 0.4041\nEpoch 52/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9309 - accuracy: 0.4822 - val_loss: 2.3574 - val_accuracy: 0.4051\nEpoch 53/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9290 - accuracy: 0.4823 - val_loss: 2.3574 - val_accuracy: 0.4042\nEpoch 54/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9278 - accuracy: 0.4860 - val_loss: 2.3574 - val_accuracy: 0.4037\nEpoch 55/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9246 - accuracy: 0.4841 - val_loss: 2.3572 - val_accuracy: 0.4047\nEpoch 56/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9253 - accuracy: 0.4831 - val_loss: 2.3574 - val_accuracy: 0.4047\nEpoch 57/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9228 - accuracy: 0.4838 - val_loss: 2.3577 - val_accuracy: 0.4046\nEpoch 58/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9245 - accuracy: 0.4842 - val_loss: 2.3572 - val_accuracy: 0.4046\nEpoch 59/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9242 - accuracy: 0.4868 - val_loss: 2.3577 - val_accuracy: 0.4043\nEpoch 60/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9279 - accuracy: 0.4839 - val_loss: 2.3575 - val_accuracy: 0.4039\nEpoch 61/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9231 - accuracy: 0.4841 - val_loss: 2.3575 - val_accuracy: 0.4044\nEpoch 62/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9209 - accuracy: 0.4855 - val_loss: 2.3573 - val_accuracy: 0.4039\nEpoch 63/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9208 - accuracy: 0.4860 - val_loss: 2.3572 - val_accuracy: 0.4048\nEpoch 64/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9245 - accuracy: 0.4823 - val_loss: 2.3575 - val_accuracy: 0.4039\nEpoch 65/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9243 - accuracy: 0.4832 - val_loss: 2.3572 - val_accuracy: 0.4042\nEpoch 66/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9265 - accuracy: 0.4817 - val_loss: 2.3575 - val_accuracy: 0.4042\nEpoch 67/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9275 - accuracy: 0.4858 - val_loss: 2.3571 - val_accuracy: 0.4047\nEpoch 68/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9262 - accuracy: 0.4821 - val_loss: 2.3577 - val_accuracy: 0.4040\nEpoch 69/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9292 - accuracy: 0.4827 - val_loss: 2.3575 - val_accuracy: 0.4043\nEpoch 70/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9238 - accuracy: 0.4836 - val_loss: 2.3575 - val_accuracy: 0.4047\nEpoch 71/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.9295 - accuracy: 0.4808 - val_loss: 2.3574 - val_accuracy: 0.4044\nEpoch 72/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9295 - accuracy: 0.4827 - val_loss: 2.3573 - val_accuracy: 0.4041\nEpoch 73/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9242 - accuracy: 0.4841 - val_loss: 2.3573 - val_accuracy: 0.4047\nEpoch 74/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9235 - accuracy: 0.4843 - val_loss: 2.3575 - val_accuracy: 0.4043\nEpoch 75/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9206 - accuracy: 0.4857 - val_loss: 2.3573 - val_accuracy: 0.4046\nEpoch 76/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9279 - accuracy: 0.4826 - val_loss: 2.3573 - val_accuracy: 0.4040\nEpoch 77/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9234 - accuracy: 0.4843 - val_loss: 2.3570 - val_accuracy: 0.4041\nEpoch 78/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9165 - accuracy: 0.4871 - val_loss: 2.3575 - val_accuracy: 0.4046\nEpoch 79/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9249 - accuracy: 0.4822 - val_loss: 2.3572 - val_accuracy: 0.4040\nEpoch 80/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9205 - accuracy: 0.4824 - val_loss: 2.3575 - val_accuracy: 0.4045\nEpoch 81/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9235 - accuracy: 0.4858 - val_loss: 2.3570 - val_accuracy: 0.4047\nEpoch 82/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9356 - accuracy: 0.4824 - val_loss: 2.3578 - val_accuracy: 0.4040\nEpoch 83/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9213 - accuracy: 0.4874 - val_loss: 2.3578 - val_accuracy: 0.4047\nEpoch 84/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9205 - accuracy: 0.4844 - val_loss: 2.3576 - val_accuracy: 0.4040\nEpoch 85/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9216 - accuracy: 0.4841 - val_loss: 2.3572 - val_accuracy: 0.4040\nEpoch 86/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9160 - accuracy: 0.4866 - val_loss: 2.3572 - val_accuracy: 0.4046\nEpoch 87/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9234 - accuracy: 0.4840 - val_loss: 2.3572 - val_accuracy: 0.4034\nEpoch 88/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9259 - accuracy: 0.4841 - val_loss: 2.3574 - val_accuracy: 0.4041\nEpoch 89/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9198 - accuracy: 0.4827 - val_loss: 2.3576 - val_accuracy: 0.4043\nEpoch 90/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9298 - accuracy: 0.4811 - val_loss: 2.3573 - val_accuracy: 0.4038\nEpoch 91/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.9253 - accuracy: 0.4831 - val_loss: 2.3576 - val_accuracy: 0.4040\nEpoch 92/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9249 - accuracy: 0.4833 - val_loss: 2.3577 - val_accuracy: 0.4050\nEpoch 93/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9272 - accuracy: 0.4848 - val_loss: 2.3575 - val_accuracy: 0.4040\nEpoch 94/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9278 - accuracy: 0.4833 - val_loss: 2.3573 - val_accuracy: 0.4041\nEpoch 95/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9305 - accuracy: 0.4812 - val_loss: 2.3573 - val_accuracy: 0.4046\nEpoch 96/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9170 - accuracy: 0.4849 - val_loss: 2.3573 - val_accuracy: 0.4042\nEpoch 97/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9202 - accuracy: 0.4830 - val_loss: 2.3577 - val_accuracy: 0.4047\nEpoch 98/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9285 - accuracy: 0.4829 - val_loss: 2.3569 - val_accuracy: 0.4049\nEpoch 99/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9279 - accuracy: 0.4828 - val_loss: 2.3577 - val_accuracy: 0.4048\nEpoch 100/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9186 - accuracy: 0.4854 - val_loss: 2.3577 - val_accuracy: 0.4043\nEpoch 101/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9248 - accuracy: 0.4837 - val_loss: 2.3574 - val_accuracy: 0.4048\nEpoch 102/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9301 - accuracy: 0.4836 - val_loss: 2.3575 - val_accuracy: 0.4049\nEpoch 103/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9229 - accuracy: 0.4844 - val_loss: 2.3574 - val_accuracy: 0.4042\nEpoch 104/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9285 - accuracy: 0.4834 - val_loss: 2.3575 - val_accuracy: 0.4047\nEpoch 105/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9239 - accuracy: 0.4844 - val_loss: 2.3571 - val_accuracy: 0.4042\nEpoch 106/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9238 - accuracy: 0.4846 - val_loss: 2.3576 - val_accuracy: 0.4044\nEpoch 107/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9212 - accuracy: 0.4864 - val_loss: 2.3571 - val_accuracy: 0.4042\nEpoch 108/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9231 - accuracy: 0.4849 - val_loss: 2.3573 - val_accuracy: 0.4042\nEpoch 109/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9214 - accuracy: 0.4841 - val_loss: 2.3575 - val_accuracy: 0.4043\nEpoch 110/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9235 - accuracy: 0.4846 - val_loss: 2.3572 - val_accuracy: 0.4043\nEpoch 111/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9185 - accuracy: 0.4845 - val_loss: 2.3573 - val_accuracy: 0.4044\nEpoch 112/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9229 - accuracy: 0.4843 - val_loss: 2.3572 - val_accuracy: 0.4041\nEpoch 113/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9265 - accuracy: 0.4818 - val_loss: 2.3572 - val_accuracy: 0.4038\nEpoch 114/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9332 - accuracy: 0.4829 - val_loss: 2.3576 - val_accuracy: 0.4051\nEpoch 115/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9202 - accuracy: 0.4832 - val_loss: 2.3575 - val_accuracy: 0.4045\nEpoch 116/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9237 - accuracy: 0.4847 - val_loss: 2.3576 - val_accuracy: 0.4046\nEpoch 117/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9233 - accuracy: 0.4853 - val_loss: 2.3575 - val_accuracy: 0.4045\nEpoch 118/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9256 - accuracy: 0.4854 - val_loss: 2.3574 - val_accuracy: 0.4039\nEpoch 119/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9203 - accuracy: 0.4865 - val_loss: 2.3575 - val_accuracy: 0.4052\nEpoch 120/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9238 - accuracy: 0.4835 - val_loss: 2.3573 - val_accuracy: 0.4043\nEpoch 121/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9221 - accuracy: 0.4839 - val_loss: 2.3571 - val_accuracy: 0.4040\nEpoch 122/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9227 - accuracy: 0.4835 - val_loss: 2.3578 - val_accuracy: 0.4038\nEpoch 123/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9318 - accuracy: 0.4819 - val_loss: 2.3574 - val_accuracy: 0.4037\nEpoch 124/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9206 - accuracy: 0.4849 - val_loss: 2.3576 - val_accuracy: 0.4041\nEpoch 125/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9309 - accuracy: 0.4812 - val_loss: 2.3571 - val_accuracy: 0.4042\nEpoch 126/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9226 - accuracy: 0.4846 - val_loss: 2.3576 - val_accuracy: 0.4044\nEpoch 127/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9227 - accuracy: 0.4848 - val_loss: 2.3570 - val_accuracy: 0.4030\nEpoch 128/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9252 - accuracy: 0.4842 - val_loss: 2.3575 - val_accuracy: 0.4035\nEpoch 129/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9257 - accuracy: 0.4840 - val_loss: 2.3572 - val_accuracy: 0.4035\nEpoch 130/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9240 - accuracy: 0.4845 - val_loss: 2.3574 - val_accuracy: 0.4045\nEpoch 131/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9224 - accuracy: 0.4866 - val_loss: 2.3572 - val_accuracy: 0.4036\nEpoch 132/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9235 - accuracy: 0.4845 - val_loss: 2.3576 - val_accuracy: 0.4043\nEpoch 133/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9195 - accuracy: 0.4860 - val_loss: 2.3571 - val_accuracy: 0.4041\nEpoch 134/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9206 - accuracy: 0.4852 - val_loss: 2.3572 - val_accuracy: 0.4045\nEpoch 135/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9293 - accuracy: 0.4842 - val_loss: 2.3578 - val_accuracy: 0.4049\nEpoch 136/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9231 - accuracy: 0.4858 - val_loss: 2.3572 - val_accuracy: 0.4053\nEpoch 137/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9221 - accuracy: 0.4856 - val_loss: 2.3575 - val_accuracy: 0.4052\nEpoch 138/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9244 - accuracy: 0.4847 - val_loss: 2.3574 - val_accuracy: 0.4043\nEpoch 139/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9310 - accuracy: 0.4818 - val_loss: 2.3577 - val_accuracy: 0.4043\nEpoch 140/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9260 - accuracy: 0.4827 - val_loss: 2.3575 - val_accuracy: 0.4041\nEpoch 141/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9192 - accuracy: 0.4845 - val_loss: 2.3574 - val_accuracy: 0.4045\nEpoch 142/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9215 - accuracy: 0.4843 - val_loss: 2.3574 - val_accuracy: 0.4042\nEpoch 143/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9239 - accuracy: 0.4844 - val_loss: 2.3571 - val_accuracy: 0.4046\nEpoch 144/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9316 - accuracy: 0.4822 - val_loss: 2.3573 - val_accuracy: 0.4041\nEpoch 145/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9245 - accuracy: 0.4863 - val_loss: 2.3572 - val_accuracy: 0.4043\nEpoch 146/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9243 - accuracy: 0.4822 - val_loss: 2.3573 - val_accuracy: 0.4042\nEpoch 147/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9240 - accuracy: 0.4846 - val_loss: 2.3568 - val_accuracy: 0.4044\nEpoch 148/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9281 - accuracy: 0.4801 - val_loss: 2.3572 - val_accuracy: 0.4040\nEpoch 149/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9282 - accuracy: 0.4837 - val_loss: 2.3573 - val_accuracy: 0.4042\nEpoch 150/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9256 - accuracy: 0.4827 - val_loss: 2.3576 - val_accuracy: 0.4046\nEpoch 151/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9255 - accuracy: 0.4801 - val_loss: 2.3571 - val_accuracy: 0.4042\nEpoch 152/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9248 - accuracy: 0.4842 - val_loss: 2.3574 - val_accuracy: 0.4040\nEpoch 153/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9239 - accuracy: 0.4834 - val_loss: 2.3573 - val_accuracy: 0.4042\nEpoch 154/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9251 - accuracy: 0.4838 - val_loss: 2.3569 - val_accuracy: 0.4044\nEpoch 155/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9202 - accuracy: 0.4838 - val_loss: 2.3572 - val_accuracy: 0.4041\nEpoch 156/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9223 - accuracy: 0.4848 - val_loss: 2.3572 - val_accuracy: 0.4043\nEpoch 157/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9279 - accuracy: 0.4821 - val_loss: 2.3576 - val_accuracy: 0.4042\nEpoch 158/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9277 - accuracy: 0.4831 - val_loss: 2.3576 - val_accuracy: 0.4045\nEpoch 159/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9193 - accuracy: 0.4857 - val_loss: 2.3573 - val_accuracy: 0.4044\nEpoch 160/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9262 - accuracy: 0.4843 - val_loss: 2.3570 - val_accuracy: 0.4040\nEpoch 161/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9231 - accuracy: 0.4840 - val_loss: 2.3572 - val_accuracy: 0.4044\nEpoch 162/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.9273 - accuracy: 0.4825 - val_loss: 2.3578 - val_accuracy: 0.4038\nEpoch 163/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9265 - accuracy: 0.4848 - val_loss: 2.3574 - val_accuracy: 0.4043\nEpoch 164/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9279 - accuracy: 0.4844 - val_loss: 2.3574 - val_accuracy: 0.4043\nEpoch 165/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9206 - accuracy: 0.4864 - val_loss: 2.3574 - val_accuracy: 0.4038\nEpoch 166/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9248 - accuracy: 0.4842 - val_loss: 2.3572 - val_accuracy: 0.4045\nEpoch 167/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9260 - accuracy: 0.4817 - val_loss: 2.3571 - val_accuracy: 0.4048\nEpoch 168/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9249 - accuracy: 0.4809 - val_loss: 2.3572 - val_accuracy: 0.4048\nEpoch 169/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9287 - accuracy: 0.4830 - val_loss: 2.3577 - val_accuracy: 0.4044\nEpoch 170/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9269 - accuracy: 0.4812 - val_loss: 2.3571 - val_accuracy: 0.4047\nEpoch 171/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9231 - accuracy: 0.4831 - val_loss: 2.3572 - val_accuracy: 0.4034\nEpoch 172/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9205 - accuracy: 0.4861 - val_loss: 2.3575 - val_accuracy: 0.4052\nEpoch 173/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.9214 - accuracy: 0.4839 - val_loss: 2.3573 - val_accuracy: 0.4047\nEpoch 174/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9229 - accuracy: 0.4852 - val_loss: 2.3573 - val_accuracy: 0.4048\nEpoch 175/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9330 - accuracy: 0.4817 - val_loss: 2.3574 - val_accuracy: 0.4036\nEpoch 176/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9269 - accuracy: 0.4808 - val_loss: 2.3571 - val_accuracy: 0.4035\nEpoch 177/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9231 - accuracy: 0.4841 - val_loss: 2.3575 - val_accuracy: 0.4043\nEpoch 178/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9236 - accuracy: 0.4856 - val_loss: 2.3572 - val_accuracy: 0.4045\nEpoch 179/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9206 - accuracy: 0.4837 - val_loss: 2.3570 - val_accuracy: 0.4042\nEpoch 180/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9261 - accuracy: 0.4817 - val_loss: 2.3570 - val_accuracy: 0.4044\nEpoch 181/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9235 - accuracy: 0.4839 - val_loss: 2.3574 - val_accuracy: 0.4045\nEpoch 182/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9253 - accuracy: 0.4846 - val_loss: 2.3571 - val_accuracy: 0.4049\nEpoch 183/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9257 - accuracy: 0.4825 - val_loss: 2.3573 - val_accuracy: 0.4040\nEpoch 184/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9270 - accuracy: 0.4837 - val_loss: 2.3575 - val_accuracy: 0.4045\nEpoch 185/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9265 - accuracy: 0.4834 - val_loss: 2.3574 - val_accuracy: 0.4042\nEpoch 186/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9230 - accuracy: 0.4835 - val_loss: 2.3573 - val_accuracy: 0.4046\nEpoch 187/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9241 - accuracy: 0.4838 - val_loss: 2.3570 - val_accuracy: 0.4041\nEpoch 188/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9256 - accuracy: 0.4841 - val_loss: 2.3574 - val_accuracy: 0.4047\nEpoch 189/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9240 - accuracy: 0.4846 - val_loss: 2.3576 - val_accuracy: 0.4049\nEpoch 190/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9209 - accuracy: 0.4835 - val_loss: 2.3573 - val_accuracy: 0.4046\nEpoch 191/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9257 - accuracy: 0.4841 - val_loss: 2.3576 - val_accuracy: 0.4042\nEpoch 192/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9217 - accuracy: 0.4845 - val_loss: 2.3574 - val_accuracy: 0.4046\nEpoch 193/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9212 - accuracy: 0.4853 - val_loss: 2.3574 - val_accuracy: 0.4049\nEpoch 194/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9305 - accuracy: 0.4836 - val_loss: 2.3577 - val_accuracy: 0.4037\nEpoch 195/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9159 - accuracy: 0.4856 - val_loss: 2.3571 - val_accuracy: 0.4044\nEpoch 196/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9248 - accuracy: 0.4825 - val_loss: 2.3569 - val_accuracy: 0.4037\nEpoch 197/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9224 - accuracy: 0.4843 - val_loss: 2.3574 - val_accuracy: 0.4043\nEpoch 198/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9254 - accuracy: 0.4861 - val_loss: 2.3571 - val_accuracy: 0.4046\nEpoch 199/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9241 - accuracy: 0.4847 - val_loss: 2.3572 - val_accuracy: 0.4045\nEpoch 200/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9262 - accuracy: 0.4827 - val_loss: 2.3577 - val_accuracy: 0.4041\nEpoch 201/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9248 - accuracy: 0.4822 - val_loss: 2.3575 - val_accuracy: 0.4043\nEpoch 202/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9267 - accuracy: 0.4826 - val_loss: 2.3574 - val_accuracy: 0.4040\nEpoch 203/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9248 - accuracy: 0.4838 - val_loss: 2.3572 - val_accuracy: 0.4038\nEpoch 204/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9207 - accuracy: 0.4834 - val_loss: 2.3576 - val_accuracy: 0.4044\nEpoch 205/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9249 - accuracy: 0.4820 - val_loss: 2.3576 - val_accuracy: 0.4042\nEpoch 206/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9229 - accuracy: 0.4839 - val_loss: 2.3573 - val_accuracy: 0.4043\nEpoch 207/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9302 - accuracy: 0.4816 - val_loss: 2.3574 - val_accuracy: 0.4043\nEpoch 208/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9231 - accuracy: 0.4852 - val_loss: 2.3576 - val_accuracy: 0.4045\nEpoch 209/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9319 - accuracy: 0.4839 - val_loss: 2.3575 - val_accuracy: 0.4044\nEpoch 210/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9251 - accuracy: 0.4873 - val_loss: 2.3571 - val_accuracy: 0.4046\nEpoch 211/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9239 - accuracy: 0.4837 - val_loss: 2.3576 - val_accuracy: 0.4042\nEpoch 212/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9233 - accuracy: 0.4850 - val_loss: 2.3570 - val_accuracy: 0.4049\nEpoch 213/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9176 - accuracy: 0.4891 - val_loss: 2.3573 - val_accuracy: 0.4040\nEpoch 214/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9161 - accuracy: 0.4860 - val_loss: 2.3579 - val_accuracy: 0.4041\nEpoch 215/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9268 - accuracy: 0.4860 - val_loss: 2.3572 - val_accuracy: 0.4043\nEpoch 216/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9285 - accuracy: 0.4820 - val_loss: 2.3572 - val_accuracy: 0.4037\nEpoch 217/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9282 - accuracy: 0.4816 - val_loss: 2.3571 - val_accuracy: 0.4044\nEpoch 218/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9257 - accuracy: 0.4839 - val_loss: 2.3572 - val_accuracy: 0.4040\nEpoch 219/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9254 - accuracy: 0.4835 - val_loss: 2.3577 - val_accuracy: 0.4040\nEpoch 220/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9214 - accuracy: 0.4843 - val_loss: 2.3571 - val_accuracy: 0.4047\nEpoch 221/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9278 - accuracy: 0.4791 - val_loss: 2.3576 - val_accuracy: 0.4040\nEpoch 222/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9250 - accuracy: 0.4826 - val_loss: 2.3574 - val_accuracy: 0.4045\nEpoch 223/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9267 - accuracy: 0.4820 - val_loss: 2.3573 - val_accuracy: 0.4040\nEpoch 224/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9207 - accuracy: 0.4823 - val_loss: 2.3577 - val_accuracy: 0.4049\nEpoch 225/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9244 - accuracy: 0.4839 - val_loss: 2.3572 - val_accuracy: 0.4045\nEpoch 226/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9179 - accuracy: 0.4840 - val_loss: 2.3575 - val_accuracy: 0.4042\nEpoch 227/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9211 - accuracy: 0.4839 - val_loss: 2.3573 - val_accuracy: 0.4036\nEpoch 228/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9261 - accuracy: 0.4854 - val_loss: 2.3572 - val_accuracy: 0.4038\nEpoch 229/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9258 - accuracy: 0.4836 - val_loss: 2.3573 - val_accuracy: 0.4045\nEpoch 230/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9222 - accuracy: 0.4860 - val_loss: 2.3575 - val_accuracy: 0.4048\nEpoch 231/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9192 - accuracy: 0.4860 - val_loss: 2.3572 - val_accuracy: 0.4040\nEpoch 232/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9250 - accuracy: 0.4846 - val_loss: 2.3574 - val_accuracy: 0.4045\nEpoch 233/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9192 - accuracy: 0.4849 - val_loss: 2.3576 - val_accuracy: 0.4045\nEpoch 234/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.9215 - accuracy: 0.4850 - val_loss: 2.3574 - val_accuracy: 0.4041\nEpoch 235/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9233 - accuracy: 0.4828 - val_loss: 2.3576 - val_accuracy: 0.4042\nEpoch 236/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9247 - accuracy: 0.4849 - val_loss: 2.3574 - val_accuracy: 0.4047\nEpoch 237/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9174 - accuracy: 0.4846 - val_loss: 2.3572 - val_accuracy: 0.4041\nEpoch 238/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9317 - accuracy: 0.4829 - val_loss: 2.3574 - val_accuracy: 0.4050\nEpoch 239/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9260 - accuracy: 0.4845 - val_loss: 2.3574 - val_accuracy: 0.4041\nEpoch 240/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9267 - accuracy: 0.4826 - val_loss: 2.3573 - val_accuracy: 0.4046\nEpoch 241/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9275 - accuracy: 0.4818 - val_loss: 2.3573 - val_accuracy: 0.4046\nEpoch 242/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9247 - accuracy: 0.4843 - val_loss: 2.3574 - val_accuracy: 0.4041\nEpoch 243/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9310 - accuracy: 0.4837 - val_loss: 2.3575 - val_accuracy: 0.4046\nEpoch 244/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.9281 - accuracy: 0.4834 - val_loss: 2.3575 - val_accuracy: 0.4042\nEpoch 245/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9214 - accuracy: 0.4860 - val_loss: 2.3574 - val_accuracy: 0.4050\nEpoch 246/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9234 - accuracy: 0.4815 - val_loss: 2.3577 - val_accuracy: 0.4044\nEpoch 247/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9258 - accuracy: 0.4844 - val_loss: 2.3577 - val_accuracy: 0.4041\nEpoch 248/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9205 - accuracy: 0.4839 - val_loss: 2.3574 - val_accuracy: 0.4039\nEpoch 249/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9240 - accuracy: 0.4817 - val_loss: 2.3575 - val_accuracy: 0.4044\nEpoch 250/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9205 - accuracy: 0.4858 - val_loss: 2.3578 - val_accuracy: 0.4047\nEpoch 251/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9275 - accuracy: 0.4838 - val_loss: 2.3574 - val_accuracy: 0.4044\nEpoch 252/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9250 - accuracy: 0.4829 - val_loss: 2.3572 - val_accuracy: 0.4045\nEpoch 253/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9209 - accuracy: 0.4834 - val_loss: 2.3577 - val_accuracy: 0.4041\nEpoch 254/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9285 - accuracy: 0.4811 - val_loss: 2.3574 - val_accuracy: 0.4044\nEpoch 255/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9313 - accuracy: 0.4804 - val_loss: 2.3577 - val_accuracy: 0.4037\nEpoch 256/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9259 - accuracy: 0.4831 - val_loss: 2.3570 - val_accuracy: 0.4046\n",
     "output_type": "stream"
    },
    {
     "execution_count": 22,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f92aa6c6410>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "score=model.evaluate(X_val,y_val)\n",
    "print(score)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:28:11.328794Z",
     "iopub.execute_input": "2023-01-25T12:28:11.329193Z",
     "iopub.status.idle": "2023-01-25T12:28:12.513053Z",
     "shell.execute_reply.started": "2023-01-25T12:28:11.329159Z",
     "shell.execute_reply": "2023-01-25T12:28:12.512131Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "text": "313/313 [==============================] - 1s 3ms/step - loss: 2.3570 - accuracy: 0.4046\n[2.356990098953247, 0.40459999442100525]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.save(filepath=f\"accuracy:{score[1]:.2f}.hdf5\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-25T12:28:35.844359Z",
     "iopub.execute_input": "2023-01-25T12:28:35.844728Z",
     "iopub.status.idle": "2023-01-25T12:28:35.948461Z",
     "shell.execute_reply.started": "2023-01-25T12:28:35.844697Z",
     "shell.execute_reply": "2023-01-25T12:28:35.947352Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": []
  }
 ]
}
